{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c36ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sra_list = [\"SRR3152141\", \n",
    "            \"SRR3152151\", \n",
    "            \"SRR3152142\",\n",
    "            \"SRR3152143\",\n",
    "            \"SRR3177945\",\n",
    "            \"SRR3152147\",\n",
    "            \"SRR3152152\",\n",
    "            \"SRR3152148\",\n",
    "            \"SRR3152150\",\n",
    "            \"SRR3178075\"]\n",
    "reference = \"crassphage.fna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c77eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000001, transformation=bowtie2-build)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000002, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000003, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000004, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000005, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000006, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000007, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000008, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000009, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000010, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000011, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000012, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000013, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000014, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000015, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000016, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000017, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000018, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000019, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000020, transformation=fasterq-dump)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000021, transformation=bowtie2)\n",
      "INFO:Pegasus.api.workflow:sra-search added Job(_id=ID0000022, transformation=merge)\n",
      "INFO:Pegasus.api.workflow:sra-search added inline TransformationCatalog\n",
      "INFO:Pegasus.api.workflow:sra-search added inline ReplicaCatalog\n",
      "INFO:Pegasus.api.workflow:inferring sra-search dependencies\n",
      "INFO:Pegasus.api.workflow:workflow sra-search with 22 jobs generated and written to workflow.yml\n",
      "\n",
      "################\n",
      "# pegasus-plan #\n",
      "################\n",
      "2022.08.01 21:53:13.282 UTC: [WARNING]  Unable to determine the version of condor\n",
      "2022.08.01 21:53:13.302 UTC: [WARNING]  Unable to determine the version of condor\n",
      "2022.08.01 21:53:13.475 UTC:\n",
      "2022.08.01 21:53:13.480 UTC:   -----------------------------------------------------------------------\n",
      "2022.08.01 21:53:13.485 UTC:   File for submitting this DAG to HTCondor           : sra-search-0.dag.condor.sub\n",
      "2022.08.01 21:53:13.491 UTC:   Log of DAGMan debugging messages                 : sra-search-0.dag.dagman.out\n",
      "2022.08.01 21:53:13.496 UTC:   Log of HTCondor library output                     : sra-search-0.dag.lib.out\n",
      "2022.08.01 21:53:13.501 UTC:   Log of HTCondor library error messages             : sra-search-0.dag.lib.err\n",
      "2022.08.01 21:53:13.506 UTC:   Log of the life of condor_dagman itself          : sra-search-0.dag.dagman.log\n",
      "2022.08.01 21:53:13.511 UTC:\n",
      "2022.08.01 21:53:13.517 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with:\n",
      "2022.08.01 21:53:13.527 UTC:   -----------------------------------------------------------------------\n",
      "2022.08.01 21:53:14.339 UTC:   Database version: '5.0.3dev' (sqlite:////home/rynge/.pegasus/workflow.db)\n",
      "2022.08.01 21:53:15.427 UTC:   Pegasus database was successfully created.\n",
      "2022.08.01 21:53:15.432 UTC:   Database version: '5.0.3dev' (sqlite:////home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0001/sra-search-0.replicas.db)\n",
      "2022.08.01 21:53:15.469 UTC:   Output replica catalog set to jdbc:sqlite:/home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0001/sra-search-0.replicas.db\n",
      "2022.08.01 21:53:16.058 UTC:   Time taken to execute is 3.124 seconds\n",
      "\n",
      "\n",
      "I have concretized your abstract workflow. The workflow has been entered \n",
      "into the workflow database with a state of \"planned\". The next step is \n",
      "to start or execute your workflow. The invocation required is\n",
      "\n",
      "\n",
      "pegasus-run /home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0001\n",
      "\n",
      "################\n",
      "# pegasus-plan #\n",
      "################\n",
      "2022.08.01 21:53:17.411 UTC: [WARNING]  Unable to determine the version of condor\n",
      "2022.08.01 21:53:17.432 UTC: [WARNING]  Unable to determine the version of condor\n",
      "2022.08.01 21:53:17.590 UTC:\n",
      "2022.08.01 21:53:17.595 UTC:   -----------------------------------------------------------------------\n",
      "2022.08.01 21:53:17.606 UTC:   File for submitting this DAG to HTCondor           : sra-search-0.dag.condor.sub\n",
      "2022.08.01 21:53:17.613 UTC:   Log of DAGMan debugging messages                 : sra-search-0.dag.dagman.out\n",
      "2022.08.01 21:53:17.626 UTC:   Log of HTCondor library output                     : sra-search-0.dag.lib.out\n",
      "2022.08.01 21:53:17.632 UTC:   Log of HTCondor library error messages             : sra-search-0.dag.lib.err\n",
      "2022.08.01 21:53:17.637 UTC:   Log of the life of condor_dagman itself          : sra-search-0.dag.dagman.log\n",
      "2022.08.01 21:53:17.642 UTC:\n",
      "2022.08.01 21:53:17.647 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with:\n",
      "2022.08.01 21:53:17.658 UTC:   -----------------------------------------------------------------------\n",
      "2022.08.01 21:53:18.441 UTC:   Database version: '5.0.3dev' (sqlite:////home/rynge/.pegasus/workflow.db)\n",
      "2022.08.01 21:53:19.498 UTC:   Pegasus database was successfully created.\n",
      "2022.08.01 21:53:19.503 UTC:   Database version: '5.0.3dev' (sqlite:////home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0002/sra-search-0.replicas.db)\n",
      "2022.08.01 21:53:19.547 UTC:   Output replica catalog set to jdbc:sqlite:/home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0002/sra-search-0.replicas.db\n",
      "[WARNING]  Submitting to condor sra-search-0.dag.condor.sub\n",
      "2022.08.01 21:53:20.148 UTC:   Time taken to execute is 3.328 seconds\n",
      "\n",
      "Your workflow has been started and is running in the base directory:\n",
      "\n",
      "/home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0002\n",
      "\n",
      "*** To monitor the workflow you can run ***\n",
      "\n",
      "pegasus-status -l /home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0002\n",
      "\n",
      "\n",
      "*** To remove your workflow run ***\n",
      "\n",
      "pegasus-remove /home/rynge/git/ACCESS-Pegasus-Examples/01-SRA-Search/rynge/pegasus/sra-search/run0002\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;32m####\u001b[0m---------------------]  15.0% ..Running (\u001b[1;34mUnready: 23\u001b[0m, \u001b[1;32mCompleted: 6\u001b[0m, \u001b[1;33mQueued: 0\u001b[0m, \u001b[1;36mRunning: 11\u001b[0m, \u001b[1;31mFailed: 0\u001b[0m)"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Sample Pegasus workflow for searching the SRA database\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = str(pathlib.Path.cwd())\n",
    "USERNAME = os.environ.get('USER')\n",
    "\n",
    "\n",
    "def add_merge_jobs(wf, parents):\n",
    "    '''\n",
    "    an upside down triangle of merge jobs to merge a set of bam\n",
    "    files into a final tarball.\n",
    "    parents is a list of jobs, for which all outputs will be\n",
    "    in the resulting tarball\n",
    "    '''\n",
    "    \n",
    "    max_parents = 25\n",
    "    final_job = False\n",
    "    level = 1\n",
    "    while len(parents) > 1:\n",
    "        children = []\n",
    "        if len(parents) <= max_parents:\n",
    "            final_job = True\n",
    "        chunks = [parents[i:i + max_parents] for i in range(0, len(parents), max_parents)]\n",
    "        job_count = 0\n",
    "        for chunk in chunks:\n",
    "            job_count += 1\n",
    "            j = Job('merge')\n",
    "            wf.add_jobs(j)\n",
    "            # outputs\n",
    "            out_file = File('results-l{}-j{}.tar.gz'.format(level, job_count))\n",
    "            if final_job:\n",
    "                out_file = File('results.tar.gz')\n",
    "            j.add_outputs(out_file, stage_out=final_job)\n",
    "            j.add_args(out_file)\n",
    "            # inputs and parent deps\n",
    "            for parent in chunk:\n",
    "                j.add_inputs(*parent.get_outputs())\n",
    "                j.add_args(*parent.get_outputs())\n",
    "            wf.add_dependency(j, parents=chunk)\n",
    "            children.append(j)\n",
    "        # next round\n",
    "        level += 1\n",
    "        parents = children\n",
    "\n",
    "\n",
    "def generate_wf(sra_list, reference):\n",
    "    '''\n",
    "    Main function that parses arguments and generates the pegasus\n",
    "    workflow\n",
    "    '''\n",
    "    \n",
    "    wf = Workflow('sra-search')\n",
    "    sc = SiteCatalog()\n",
    "    tc = TransformationCatalog()\n",
    "    rc = ReplicaCatalog()\n",
    "    \n",
    "    # --- Properties ----------------------------------------------------------\n",
    "\n",
    "    props = Properties()\n",
    "     # set the concurrency limit for the download jobs\n",
    "    props['dagman.fasterq-dump.maxjobs'] = '20'\n",
    "    # send some extra usage stats to the Pegasus developers\n",
    "    props['pegasus.catalog.workflow.amqp.url'] = 'amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows'\n",
    "    props.write()\n",
    "    \n",
    "    # --- Sites ---------------------------------------------------------------\n",
    "    \n",
    "    shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "    local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "    local = Site(\"local\")\\\n",
    "      .add_directories(\n",
    "          Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "            .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "          Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "            .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\\\n",
    "      .add_pegasus_profile(pegasus_lite_env_source=\"{}/tools/job-env-setup.sh\".format(BASE_DIR))\n",
    "    sc.add_sites(local)\n",
    "    sc.write()\n",
    "\n",
    "    # --- Transformations -----------------------------------------------------\n",
    "    \n",
    "    container = Container(\n",
    "                   'sra-search',\n",
    "                   Container.SINGULARITY,\n",
    "                   'docker://pegasus/sra-search:latest'\n",
    "                )\n",
    "    tc.add_containers(container)\n",
    "\n",
    "    bowtie2_build = Transformation(\n",
    "                       'bowtie2-build',\n",
    "                       site='incontainer',\n",
    "                       container=container,\n",
    "                       pfn='/opt/bowtie2-2.2.9/bowtie2-build',\n",
    "                       is_stageable=False\n",
    "                    )\n",
    "    bowtie2_build.add_profiles(Namespace.CONDOR, key='request_memory', value='1 GB')\n",
    "    #bowtie2_build.add_profiles(Namespace.CONDOR, key='+TargetAnnexName', value='\"{}\"'.format(USERNAME))\n",
    "    tc.add_transformations(bowtie2_build)\n",
    "    \n",
    "    bowtie2 = Transformation(\n",
    "                  'bowtie2',\n",
    "                  site='local',\n",
    "                  container=container,\n",
    "                  pfn=BASE_DIR + '/tools/bowtie2_wrapper',\n",
    "                  is_stageable=True\n",
    "              )\n",
    "    bowtie2.add_profiles(Namespace.CONDOR, key='request_memory', value='2 GB')\n",
    "    #bowtie2.add_profiles(Namespace.CONDOR, key='+TargetAnnexName', value='\"{}\"'.format(USERNAME))\n",
    "    tc.add_transformations(bowtie2)\n",
    "\n",
    "    fasterq_dump = Transformation(\n",
    "                      'fasterq-dump',\n",
    "                       site='local',\n",
    "                       container=container,\n",
    "                       pfn=BASE_DIR + '/tools/fasterq_dump_wrapper',\n",
    "                       is_stageable=True\n",
    "                     )\n",
    "    fasterq_dump.add_profiles(Namespace.CONDOR, key='request_memory', value='1 GB')\n",
    "    #fasterq_dump.add_profiles(Namespace.CONDOR, key='+TargetAnnexName', value='\"{}\"'.format(USERNAME))\n",
    "    # this one is used to limit the number of concurrent downloads\n",
    "    fasterq_dump.add_profiles(Namespace.DAGMAN, key='category', value='fasterq-dump')\n",
    "    tc.add_transformations(fasterq_dump)\n",
    "\n",
    "    merge = Transformation(\n",
    "                'merge',\n",
    "                site='local',\n",
    "                container=container,\n",
    "                pfn=BASE_DIR + '/tools/merge',\n",
    "                is_stageable=True\n",
    "            )\n",
    "    merge.add_condor_profile(request_memory='1 GB')\n",
    "    tc.add_transformations(merge)\n",
    "\n",
    "\n",
    "    # --- Workflow -----------------------------------------------------\n",
    "\n",
    "    # keep track of bam files, so we can merge them into a single tarball at\n",
    "    # the end\n",
    "    to_merge = []\n",
    "\n",
    "    # set up reference file and what files needs to be generated by the index job\n",
    "    ref_main = File('reference.fna')\n",
    "    rc.add_replica('local', 'reference.fna', os.path.abspath(reference))\n",
    "    ref_files = []\n",
    "    for filename in ['reference.1.bt2', 'reference.2.bt2', 'reference.3.bt2', 'reference.4.bt2',\n",
    "                     'reference.rev.1.bt2', 'reference.rev.2.bt2']:\n",
    "        ref_files.append(File(filename))\n",
    "\n",
    "    # index the reference file\n",
    "    index_job = Job('bowtie2-build')\n",
    "    index_job.add_args('reference.fna', 'reference')\n",
    "    index_job.add_inputs(ref_main)\n",
    "    index_job.add_outputs(*ref_files, stage_out=False)\n",
    "    wf.add_jobs(index_job)\n",
    "\n",
    "    # create jobs for each SRA ID\n",
    "    for sra_id in sra_list:\n",
    "        if len(sra_id) < 5:\n",
    "            continue\n",
    "\n",
    "        # files for this id\n",
    "        fastq_1 = File('{}_1.fastq'.format(sra_id))\n",
    "        fastq_2 = File('{}_2.fastq'.format(sra_id))\n",
    "\n",
    "        # download job\n",
    "        j = Job('fasterq-dump')\n",
    "        j.add_args('--split-files', sra_id)\n",
    "        j.add_outputs(fastq_1, fastq_2, stage_out=False)\n",
    "        wf.add_jobs(j)\n",
    "\n",
    "        # bowtie2 job\n",
    "        bam = File('{}.bam'.format(sra_id))\n",
    "        bam_index = File('{}.bam.bai'.format(sra_id))\n",
    "        j = Job('bowtie2')\n",
    "        j.add_args(sra_id)\n",
    "        j.add_inputs(*ref_files, fastq_1, fastq_2)\n",
    "        j.add_outputs(bam, bam_index, stage_out=False)\n",
    "        wf.add_jobs(j)\n",
    "        \n",
    "        # keep track of jobs and outputs for merging\n",
    "        to_merge.append(j)\n",
    "    \n",
    "    add_merge_jobs(wf, to_merge)\n",
    "\n",
    "    try:\n",
    "        wf.add_transformation_catalog(tc)\n",
    "        wf.add_replica_catalog(rc)\n",
    "        wf.plan()\n",
    "    except PegasusClientError as e:\n",
    "        print(e.output)\n",
    "    return wf\n",
    "\n",
    "\n",
    "wf = generate_wf(sra_list, reference)\n",
    "\n",
    "try:\n",
    "    wf.plan(submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4811bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
