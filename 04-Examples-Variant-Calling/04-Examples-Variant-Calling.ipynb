{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant Calling Pegasus Workflow\n",
    "\n",
    "In this notebook, we will create Pegasus Workflow corresponding to the  \n",
    "[Automating a Variant Calling Workflow](https://datacarpentry.org/wrangling-genomics/05-automation/index.html) \n",
    "from Data Carpentry Lesson \n",
    "[Data Wrangling and Processing for Genomics](https://datacarpentry.org/wrangling-genomics/).\n",
    "\n",
    "This workflow downloads and aligns SRA data to the E. coli  REL606 reference genome, and checks what differences exist in our reads versus the genome. The workflow also performs perform  variant calling to see how \n",
    "the population changed over time. \n",
    "\n",
    "One major change from other examples is that this workflow, uses Open Storage Network (OSN) to do the\n",
    "data staging. The previous examples, uses the directory space on **match.pegasus.isi.edu** which is\n",
    "limited.\n",
    "\n",
    "## Container\n",
    "\n",
    "All tools required to execute the jobs in the container are all included in\n",
    "a single Docker container defined in `docker/Dockerfile` and available in the\n",
    "[Docker Hub](https://hub.docker.com/repository/docker/pegasus/variant-calling) under\n",
    "`pegasus/variant-calling`. The workflow is setup up to use that container\n",
    "but execute it via Singularity as that maybe a more common container\n",
    "runtime on HPC machines. The container runtime used can easily be\n",
    "changed in the workflow definition.\n",
    "\n",
    "The container comes with the following tools\n",
    "* Burrows-Wheeler Aligner (BWA) 0.7.17\n",
    "* SamTools 1.15.1\n",
    "* Bcftools 1.15.1\n",
    "* HTSlib   1.15.1\n",
    "* SRA Tools 3.0.0\n",
    "\n",
    "The number of concurrent downloads is limited with a DAGMan\n",
    "category profile.\n",
    "\n",
    "## OSN Access\n",
    "\n",
    "The Open Storage Network (OSN) is a distributed data sharing and transfer service intended to facilitate exchanges of active scientific data sets between research organizations, communities and projects, providing easy access and high bandwidth delivery of large data sets to researchers.\n",
    "\n",
    "The OSN serves two principal purposes: (1) enable the smooth flow of large data sets between resources such as instruments, campus data centers, national supercomputing centers, and cloud providers; and (2) facilitate access to long tail data sets by the scientific community.  \n",
    "\n",
    "If you dont have access to an existing OSN bucket. Please refer to the instructions in the \n",
    "[XSEDE user guide](https://portal.xsede.org/osn-user-guide)\n",
    "\n",
    "## Workflow\n",
    "\n",
    "The Pegasus workflow downloads SRA data from NCBI repository using\n",
    "`fasterq-dump` in the SRA toolkit and aligns it against the reference \n",
    "genome.\n",
    "\n",
    "![Pegasus Variant Calling Workflow for 2 SRA reads](../images/variant-calling-pegasus-workflow.png)\n",
    "\n",
    "The tools used for various jobs in the worklfow are listed in table below\n",
    "\n",
    "| Job Label                 | Tool Used        |\n",
    "| --------------------------|----------------- |\n",
    "| fasterq_dump              | fasterq_dump     |\n",
    "| align_reads               | bwa              |\n",
    "| sam_2_bam_converter       | samtools         |\n",
    "| calculate_read_coverage   | bcftools         |\n",
    "| detect_snv                | bcftools         |\n",
    "| variant_calling           | vcfutils         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up Credentials for OSN\n",
    "\n",
    "As a first step, you need to specify your access key and secret key in the Pegasus credentials file. \n",
    "\n",
    "This file resides at ~/.pegasus/credentials.conf\n",
    "\n",
    "Open this file in your favorite editor (vim, emacs, nano) and put in the following\n",
    "\n",
    "```\n",
    "\n",
    "[osn]\n",
    "endpoint = https://sdsc.osn.xsede.org\n",
    "\n",
    "[USER@osn]\n",
    "access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "secret_key = abababababababababababababababab\n",
    "\n",
    "```\n",
    "\n",
    "**Note** Replace USER with your ACCESS username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.pegasus/credentials.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Logging\n",
    "\n",
    "Configure logging. While this is **not required**, it is useful for seeing output from tools such as `pegasus-plan`, `pegasus-analyzer`, etc. when using these python wrappers. Here we also include a few other imports we might need further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = Path(\".\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Pegasus Properties\n",
    "\n",
    "The `pegasus.properties` file can now be generated using the `Properties()` object as shown below. To see a list of the most commonly used properties, you can use `Properties.ls(prefix)`. By default, `pegasus-plan` will look in `cwd` for a `pegasus.properties` file if one is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"                                                                    \n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.mode\"] = \"tutorial\" # speeds up tutorial workflows - remove for production ones\n",
    "props.write() # written to ./pegasus.properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Properties.ls(\"condor.request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Replica Catalog (Specify Initial Input Files)\n",
    "\n",
    "Any initial input files given to the workflow should be specified in the `ReplicaCatalog`. This object tells Pegasus where each input file is physically located. First, we create a file that will be used as input to this workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"f.a\", \"w\") as f:\n",
    "    f.write(\"This is the contents of the input file for the diamond workflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `./f.a` will be used in this workflow, and so we create a corresponding `File` object. Metadata may also be added to the file as shown below.\n",
    "\n",
    "Next, a `ReplicaCatalog` object is created so that the physical locations of each input file can be cataloged. This is done using the `ReplicaCatalog.add_replica(site, file, path)` function. As the file `f.a` resides here on the submit machine, we use the reserved keyword `local` for the site parameter. Second, the `File` object is passed in for the `file` parameter. Finally, the absolute path to the file is given. `pathlib.Path` may be used as long as an absolute path is given. \n",
    "\n",
    "By default, `pegasus-plan` will look in `cwd` for a `replicas.yml` file if one is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replicas -----------------------------------------------------------------\n",
    "fa = File(\"f.a\").add_metadata(creator=\"ryan\")\n",
    "rc = ReplicaCatalog()\\\n",
    "    .add_replica(\"local\", fa, Path(\".\").resolve() / \"f.a\")\\\n",
    "    .write() # written to ./replicas.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat replicas.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Transformation Catalog (Specify Executables Used)\n",
    "\n",
    "Any executable (referred to as ***transformations***) used by the workflow needs to be specified in the `TransformationCatalog`. This is done by creating `Transformation` objects, which represent executables. Once created, these must be added to the `TransformationCatalog` object. \n",
    "\n",
    "By default, `pegasus-plan` will look in `cwd` for a `transformations.yml` file.\n",
    "\n",
    "For ACCESS, we recommend users specify containers in which their jobs run in. This allows you to\n",
    "have similar environment in which jobs run irrespective of the ACCESS resource on which \n",
    "the job is launched.\n",
    "\n",
    "In Pegasus, users have the option of either using a different container for each executable or same container for all executables. When using containers with Pegasus you have two options\n",
    "\n",
    "1. The container has your executables pre installed. In that case in your transformation catalog, you specify the PFN as the path in the container where your executable is accessible\n",
    "\n",
    "2. The other case, is you are using a generic baseline container and want to let Pegasus stage your executables in at runtime. To do that you can mark the executable as **stageable** (is_stageable as True) and Pegasus will stage the executable into the container, as part of executable staging.\n",
    "\n",
    "In the example below, we are indicating that the preprocess, findrange and analyze executables need a container named *base_container* to run. However, we are going to let Pegasus stage them into container when your workflow runs from their location on site `condorpool` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Container ----------------------------------------------------------\n",
    "\n",
    "base_container = Container(\n",
    "                  \"base-container\",\n",
    "                  Container.SINGULARITY,\n",
    "                  image=\"docker://karanvahi/pegasus-tutorial-minimal\"\n",
    "    \n",
    "                  # comment out the location below (and comment the above location) \n",
    "                  # if you run into docker rate pull limits. Do this if your  \n",
    "                  # workflow fails on the first try with stage-in jobs fail \n",
    "                  # with error like ERROR: toomanyrequests: Too Many Requests. OR\n",
    "                  # You have reached your pull rate limit. You may increase \n",
    "                  # the limit by authenticating and upgrading: \n",
    "                  # ttps://www.docker.com/increase-rate-limits. \n",
    "                  # You must authenticate your pull requests.\n",
    "                  #\n",
    "                  # This is why Pegasus supports tar files of containers, \n",
    "                  # and also ensures the pull from a docker hub happens only \n",
    "                  # once per workflow\n",
    "    \n",
    "                  #image=\"http://download.pegasus.isi.edu/pegasus/tutorial/pegasus-tutorial-minimal.tar.gz\"\n",
    "               )\n",
    "\n",
    "\n",
    "# --- Transformations ----------------------------------------------------------\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "findrange = Transformation(\n",
    "                \"findrange\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "analyze = Transformation(\n",
    "                \"analyze\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_containers(base_container)\\\n",
    "    .add_transformations(preprocess, findrange, analyze)\\\n",
    "    .write() # written to ./transformations.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat transformations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the container is listed once, and multiple transformations can refer to the same container.\n",
    "\n",
    "Some attributes to keep an eye out for\n",
    "- *name*  the name assigned to the container that is used as a reference handle when describing executables in Transformation\n",
    "\n",
    "- *type*  type of Container. Usually is Dokcer or Singularity\n",
    "\n",
    "- *image* - URL to image in a docker|singularity hub or URL to an existing docker image exported as a tar file or singularity image.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create a Site Catalog\n",
    "\n",
    "A Site Catalog allows you to describe to Pegasus what your sites look alike. By default, Pegasus always\n",
    "creates two default sites\n",
    "\n",
    "* local - it is used to indicate the workflow submit node from where you are issuing pegasus commands. **local** site is usually used to run only data management tasks that Pegasus adds to the workflow. The users compute jobs are not executed on this site.\n",
    "* condorpool - it is used to indicate the default execution site that consists of condor workers. For this tutorial, the **condorpool** site will be composed of condor workers launched by pilot jobs on ACCESS sites in Section 10.\n",
    "\n",
    "In this tutorial, we create a local site mainly to specify a job environment setup file, that gets sourced before a job runs on an ACCESS resources, and loads all the relevant modules for the job (namely singularity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sites -----------------------------------------------------------------\n",
    "# add a local site with an optional job env file to use for compute jobs\n",
    "shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "\n",
    "local = Site(\"local\") \\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\n",
    "\n",
    "job_env_file = Path(str(BASE_DIR) + \"/../tools/job-env-setup.sh\").resolve()\n",
    "local.add_pegasus_profile(pegasus_lite_env_source=job_env_file)\n",
    "\n",
    "sc = SiteCatalog()\\\n",
    "   .add_sites(local)\\\n",
    "   .write() # written to ./sites.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat sites.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create the Workflow\n",
    "\n",
    "The `Workflow` object is used to store jobs and dependencies between each job. Typical job creation is as follows:\n",
    "\n",
    "```\n",
    "# Define job Input/Output files\n",
    "input_file = File(\"input.txt\")\n",
    "output_file1 = File(\"output1.txt\")\n",
    "output_file2 = File(\"output2.txt\")\n",
    "\n",
    "# Define job, passing in the transformation (executable) it will use\n",
    "j = Job(transformation_obj)\n",
    "\n",
    "# Specify command line arguments (if any) which will be passed to the transformation when run\n",
    "j.add_args(\"arg1\", \"arg2\", input_file, \"arg3\", output_file)\n",
    "\n",
    "# Specify input files (if any)\n",
    "j.add_inputs(input_file)\n",
    "\n",
    "# Specify output files (if any)\n",
    "j.add_outputs(output_file1, output_file2)\n",
    "\n",
    "# Add profiles to the job\n",
    "j.add_env(FOO=\"bar\")\n",
    "j.add_profiles(Namespace.PEGASUS, key=\"checkpoint.time\", value=1)\n",
    "\n",
    "# Add the job to the workflow object\n",
    "wf.add_jobs(j)\n",
    "```\n",
    "\n",
    "By default, depedencies between jobs are inferred based on input and output files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"blackdiamond\")\n",
    "\n",
    "fb1 = File(\"f.b1\")\n",
    "fb2 = File(\"f.b2\")\n",
    "job_preprocess = Job(preprocess)\\\n",
    "                    .add_args(\"-a\", \"preprocess\", \"-T\", \"3\", \"-i\", fa, \"-o\", fb1, fb2)\\\n",
    "                    .add_inputs(fa)\\\n",
    "                    .add_outputs(fb1, fb2)\n",
    "\n",
    "fc1 = File(\"f.c1\")\n",
    "job_findrange_1 = Job(findrange)\\\n",
    "                    .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb1, \"-o\", fc1)\\\n",
    "                    .add_inputs(fb1)\\\n",
    "                    .add_outputs(fc1)\n",
    "\n",
    "fc2 = File(\"f.c2\")\n",
    "job_findrange_2 = Job(findrange)\\\n",
    "                    .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb2, \"-o\", fc2)\\\n",
    "                    .add_inputs(fb2)\\\n",
    "                    .add_outputs(fc2)\n",
    "\n",
    "fd = File(\"f.d\")\n",
    "job_analyze = Job(analyze)\\\n",
    "                .add_args(\"-a\", \"analyze\", \"-T\", \"3\", \"-i\", fc1, fc2, \"-o\", fd)\\\n",
    "                .add_inputs(fc1, fc2)\\\n",
    "                .add_outputs(fd)\n",
    "\n",
    "wf.add_jobs(job_preprocess, job_findrange_1, job_findrange_2, job_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Workflow\n",
    "\n",
    "Once you have defined your abstract workflow, you can use `pegasus-graphviz` to visualize it. `Workflow.graph()` will invoke `pegasus-graphviz` internally and render your workflow using one of the available formats such as `png`. **Note that Workflow.write() must be invoked before calling Workflow.graph().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run the Workflow\n",
    "\n",
    "When working in Python, we can just use the reference do the `Workflow` object, you can plan, run, and monitor the workflow directly. These are wrappers around Pegasus CLI tools, and as such, the same arguments may be passed to them. \n",
    "\n",
    "**Note that the Pegasus binaries must be added to your PATH for this to work.**\n",
    "\n",
    "Please wait for the progress bar to indicate that the workflow has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line in the output that starts with pegasus-status, contains the command you can use to monitor the status of the workflow. We will cover this command line tool in the next couple of notbooks. The path it contains is the path to the submit directory where all of the files required to submit and monitor the workflow are stored. For now we will just continue to use the Python `Workflow` object.\n",
    "\n",
    "## 10.  Launch Pilots Jobs on ACCESS resources\n",
    "\n",
    "At this point you should have some idle jobs in the queue. They are idle because there are no resources yet to execute on. Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the ACCESS resource providers. These pilots have the following properties:\n",
    "\n",
    "A pilot can run multiple user jobs - it stays active until no more user jobs are available or until end of life has been reached, whichever comes first.\n",
    "\n",
    "A pilot is partitionable - job slots will dynamically be created based on the resource requirements in the user jobs. This means you can fit multiple user jobs on a compute node at the same time.\n",
    "\n",
    "A pilot will only run jobs for the user who started it.\n",
    "\n",
    "You have to have an allocation at the resource provider you want to use. The resources we currently support are:\n",
    "* TACC Stampede2\n",
    "* SDSC Expanse\n",
    "* PSC Bridges2\n",
    "* Purdue Anvil\n",
    "\n",
    "### 10.1 Starting an annex\n",
    "\n",
    "In order to start an annex, you need to open a shell terminal via the notebook. To do this\n",
    "\n",
    "1. Click on the Jupyter Icon on top left of your window.\n",
    "2. In the new page, click on the New Button towards Top Right of the Window. \n",
    "3. Select the terminal option\n",
    "\n",
    "In the open terminal you will run the following command\n",
    "\n",
    "`htcondor annex create --project PROJECT_ID $USER QUEUE@RESOURCE`\n",
    "\n",
    "Before running the above command, you need to to identify the following for the ACCESS Resource you \n",
    "want the workflow to run on.\n",
    "\n",
    "* PROJECT_ID : The project allocation\n",
    "* QUEUE: The queue to which you want the pilot jobs to be submitted\n",
    "* RESOURCE: The name of the ACCESS Resource. Can be one of stampede2, anvil, expanse, psc\n",
    "\n",
    "For more information on how to determine your project id and queue on an ACCESS resource, please\n",
    "visit [this page](https://confluence.pegasus.isi.edu/display/pegasus/ACCESS+Pegasus#ACCESSPegasus-HTCondorPool/Annex)\n",
    "\n",
    "For example, to create an annex on Stampede2, you will do \n",
    "\n",
    "`htcondor annex add --project TG-XXXXXX --nodes 1 $USER development@stampede2`\n",
    "\n",
    "## 11. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. What's Next?\n",
    "\n",
    "To continue exploring Pegasus, and specifically learn how to debug failed workflows, please open the notebook in `02-Debugging/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
